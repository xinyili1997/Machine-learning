Lecture3
	https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=bb73c204-36af-4d6a-ba1e-a9d100ff9027
The main goal today(Lecture3) is to talk about how to actually do decision tree learning.

<1> An Aside:
      1.1 The majority vote classifier <2 procedues>
	def train(D):
	      store(D)
	      V = majority_class(D)		#the class that appears most often in D
	def prediction(x):
	      return V
      1.2 Definition for Error rate
            1.2.1 dataset and classifier	
	* Let D = {(x(1), y(1)), ... , (x(N), y(N))} be a dataset
	* And h(x): X -> Y be a classifier	#a function that predict the result from X to Y
            1.2.2 error rate function
	* error(h, D) = 1/N * sum_(from i=1 to N) of Indicator_function(h(x)』y(i))
	   #where indicator function is just like the 0/1 loss function
	   #y(i) = c*(x)
      1.3  Indicator function
	* Indicator_function(logical proposation)
	   - if True, ruturn 1
	   - if False, return 0
------------------------------------------------------------------------------------------
<2> Decision Tree Learning <KEY FUNCTION>
      2.1 Decision Tree Learning
	def train(D):
	      root = newNode(data = D)
	      train_tree(root)
	def train_tree(node):
	      - <1> m = best attribute on which to spilt nodes
	      - <2> Let m be decision attribute for node
	      - <3> For each value of m, creates descendant node (or child node)
	      - <4> Position node's data into descendants
	      - <5> if data in descendant is perfectly classified:
		      break (or stop)
		else:
		      recurse on descendant
            #Question1:
	- What is "perfectly classified" in step <5> ?
	- The error rate of the actual data on that node is zero, error(h, D) = 0,
		(Presupposes we use a majority vite at each leaf)
            #Question2:
	- How to find "best attribute" in step <1>, what is our notion for "best" ?
	- We have 3 options here:
	      1. error rate (highest)
	      2. mutual information (highest)
	      3. gini impurity (highest)
------------------------------------------------------------------------------------------
      2.2 EX. DT learning (Output Y, Attributes A, B, C)   slides 12
      #STEP1: computer error rate for attributes A, B and C to find the best attribute
	* hA option: [A]
		if A = 0 -----> [1+, 0-] & majority vote is positive + -----> 0- error
		if A = 1 -----> [4+, 3-] & majority vote is positive + -----> 3- error
	* hB option: [B]
		if B = 0 -----> [1+, 3-] & majority vote is negative - -----> 1+ error
		if B = 1 -----> [4+, 0-] & majority vote is positive + -----> 0- error
	* hC option: [C]
		if C = 0 -----> [2+, 2-] & majority vote is pos/neg +/- -----> 2+/- error
		if C = 1 -----> [3+, 1-] & majority vote is positive + -----> 1- error
	* 
	#error(hA, D_root) = 3/8;
	#error(hB, D_root) = 1/8;
	#error(hC, D_root) = 3/8;
	*
	Thus, B node is the best attribute!
      #STEP2: recurse on A and C node
	* 1st attribue [B]
		if B = 0 -----> [1+, 3-] -----> create descendant node with A or C
		if B = 1 -----> [4+, 0-] -----> 0- error <FIXED>
	* find decendant node with A and C
		* hA option: [A]
			if A = 0 -----> [1+, 0-] & majority vote is positive + -----> 0- error <FIXED>
			if A = 1 -----> [0+, 3-] & majority vote is negative - -----> 0+ error <FIXED>
		* hC option: [C]
			if C = 0 -----> [0+, 2-] & majority vote is negative - -----> 0- error
			if C = 1 -----> [1+, 1-] & majority vote is pos/neg +/- -----> 1-/+ error
	* 
	#error(hA, hB_root) = 0;
	#error(hC, hB_root) = 1/4;
	*
	Thus, A node is the best 2nd attribute!
      #STEP3: recursion or stop
	* known that A node descendent is perfectly classified, then stop.
	* So that the DT is built
	               [B]
	               / \
	             0    1
	            /        \
	          [A]   <fixed>
	          / \
	        0     1
	      /         \
                <fixed> <fixed>
<3> Information Theory & DTs
-----------------------------------------------------------------------------------------------
      3.1 Information Theory <KEY DEFINITION>
            >>> Let X be a random variable, x （ X
            >>> Let Y be a random variable, y （ Y
	* (Margainal) Entropy: 
		H(Y) = -\sum_(y（Y) [ P(Y=y) * log_2(P(Y=y)) ]
	* (Specific) Conditional Entropy: 
		H(Y|X=x) = -\sum_(y（Y) [ P(Y|X=x) * log_2(P(Y|X=x)) ]
	* Conditional Entropy: 
		H(Y|X) = \sum_(x（X) [ P(X=x) * P(Y|X=x) ]
	* Mutual Information (Information gain): 
		I(Y;X) = H(Y) - H(Y|X)
		         = H(X) - H(X|Y)
-----------------------------------------------------------------------------------------------
      3.2 EX. find attribute based on information gain   slides 13
      #STEP0: prepare DT and possibility P
	* DT
		[6+, 2-]		[6+, 2-]
		   [A]		   [B]
		   /  \		   /  \
		 0     1		 0     1
		/        \		/        \
   	         [0+, 0-]   [6+, 2-]      [2+, 2-]   [4+, 0-]
	* possibility P
	   - Outputs 
		* for Y: P(Y=0) = 2/8; P(Y=1) = 6/8
	   - Attributes X:
		* for A: P(A=0) = 0; P(A=1) = 1 
		* for B: P(B=0) = 1/2; P(B=1) = 1/2   
      #STEP1: computer (Margainal) Entropy H(Y)
	* H(Y) = 2/8*log2(2/8) + 6/8*log2(6/8)
      #STEP2: computer information gain for A and B
            2.1 case#A
	* H(Y|A=0) = "undefined"
	* H(Y|A=1) = 2/8*log2(2/8) + 6/8*log2(6/8) = H(Y)
	* H(Y|A) = P(A=0)*H(Y|A=0) + P(A=1)*H(Y|A=1) = H(Y)
	* I(Y;A) = H(Y) - H(Y|A) = 0
           2.2 case#B
	* H(Y|B=0) = 1/2*log2(1/2) + 1/2*log2(1/2)
	* H(Y|B=1) = 0*log2(0) + 1*log2(1) = 0
	* H(Y|B) = 1/2*H(Y|B=0) + 1/2*(0)
	* I(Y;B) = H(Y) - H(Y|B) > 0
      #STEP3: compare and use Information Gain as DT splitting criterion
	* known that I(Y;A) = 0 and I(Y;B) > 0, I(Y;A) < I(Y;B).
	* Thus, B is better!!! 
<4> HW reminders
      * hw2: Decision Trees
	- Out: Wed, Jan 23 (3nd lecture)
	- Due: Wed, Feb 6 at 11:59pm
